Direct API calls to OpenAI

API calls through LangChain:

- Prompts
- Models
- Output parsers

Tasks

- Get your OpenAI API Key
- Configure OpenAI API Key using python-dotenv
- Model selection example to account for deprecation of LLM model
- OpenAI API call for the prompt: 1 + 1
- OpenAI API call for converting Pirate to English with given style
- LangChain API call example for using prompt template. Convert Pirate to English with given style
- LangChain API call example for using prompt template. Convert English to Pirate with given style
- Output Parser
	- Define the output format in JSON
	- Initialize the customer review text
	- Create a review template with place holders for values extracted from customer review text
	- Parse the LLM output string into Python dictionary
	- Print the parsed output



**Install and Import Required Libraries**:  

  Installation of `python-dotenv` and `openai` libraries. The code will load environment variables (like the API key) from a `.env` file and set `openai.api_key`.

**Note on LLM Results**:  

   LLMs can return slightly different results each time they run, so results may vary.

**Model Selection Logic**:  

   Shows a logic snippet that, based on the current date, selects one OpenAI model version over another.

**Chat API: OpenAI**:  

   A function `get_completion` makes a chat completion request to the OpenAI API. The example input prompt is “What is 1+1?” and it returns the completion from the model.

**Translating Customer Email**:  

   Defines a `customer_email` variable containing text in a certain style and a `style` variable defining the desired output style. Demonstrates constructing a prompt that asks the LLM to translate the email into the specified style.

**Printing and Getting the Response**:  

   Shows how the prompt is printed and how the `get_completion` function is called to get the response from the LLM.

**Chat API: LangChain**:  

   Introduces LangChain as a library to interact with models, create prompts, and parse outputs in a structured way—similar tasks done previously with direct OpenAI API calls.

**Models (LangChain)**:  

   Explain how to initialize a LangChain `ChatOpenAI` model instance with parameters like `temperature` and `model`.

**Prompt Template (LangChain)**:  

   Define a template string that instructs how the text should be translated. Shows how to create a `ChatPromptTemplate` from the template and retrieve its input variables.

**Formatting Messages with the Template**:  

   Demonstrates how to insert variables (`style` and `text`) into the prompt template to generate the final messages passed to the model.

**Generating Responses via LangChain**:  

   Shows how to call the model (e.g., `chat(customer_messages)`) and obtain the transformed text in the desired style.

**Changing Styles**:  

   Provides another example by defining a new style and rewriting a service response into that style (e.g., English Pirate).

**Output Parsers**:  

   Introduces the concept of defining a structured format for the output (e.g., JSON) and asking the LLM to return responses in that structure.

**Customer Review Example**:  

  Provides a text snippet (customer review) and a template that instructs the LLM to extract specific information (gift, delivery_days, price_value) from the text and format it as JSON.

**Using Response Schemas**:  

  Shows how to define schemas using `ResponseSchema` that describe the expected fields in the output. Demonstrates how to instruct the LLM to return data matching these schemas.

**StructuredOutputParser**:  

  Explains the `StructuredOutputParser` which uses the defined response schemas. The parser provides format instructions that can be included in the prompt so the LLM’s response can be easily parsed into a Python dictionary.

**Formatting Instructions**:  

   Shows the creation of `format_instructions` and how they are integrated into a revised template prompt. This prompts the LLM to return data in a clearly defined, machine-readable JSON structure.

## Model Selection Logic

Model selection logic allows you to adapt to changes in model availability or versioning over time. For example, if a specific model version is scheduled to be deprecated or replaced by another after a certain date, the logic ensures your code automatically switches to the newer model. This approach helps maintain compatibility, reliability, and the best possible performance from the models without requiring manual updates.

Hard-coding values in code is often done for quick demonstrations, prototyping, or when the logic is simple and unlikely to change frequently. In a tutorial or example scenario, showing the logic directly in the code provides a clear, self-contained example without depending on external files.

However, hard-coded logic is not a best practice for production environments. Storing such configurations in a separate configuration file or environment variable is usually better. It makes the logic more flexible, easier to maintain, and simpler to update without altering the code.

